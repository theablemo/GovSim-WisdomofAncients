{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GovSim Metrics Analysis\n",
                "\n",
                "This notebook analyzes the metrics from GovSim simulations. It consists of two main sections:\n",
                "1. Generating metrics for individual simulations\n",
                "2. Comparing metrics across different simulations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import os\n",
                "import json\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "\n",
                "# Set up visualization style\n",
                "plt.style.use('ggplot')\n",
                "sns.set_theme(style=\"whitegrid\")\n",
                "\n",
                "# Define results directory\n",
                "RESULTS_DIR = Path('../results')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Analyzing Individual Simulations\n",
                "\n",
                "This section loads and analyzes data from individual simulation runs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def load_simulation_data(simulation_dir):\n",
                "    \"\"\"Load data from a simulation directory\"\"\"\n",
                "    sim_path = RESULTS_DIR / simulation_dir\n",
                "    \n",
                "    # Load metrics summary if available\n",
                "    metrics_summary_path = sim_path / \"metrics_summary.json\"\n",
                "    metrics_summary = None\n",
                "    if metrics_summary_path.exists():\n",
                "        with open(metrics_summary_path, 'r') as f:\n",
                "            metrics_summary = json.load(f)\n",
                "    \n",
                "    # Load detailed metrics data\n",
                "    metrics_data_path = sim_path / \"metrics_data.csv\"\n",
                "    metrics_data = None\n",
                "    if metrics_data_path.exists():\n",
                "        metrics_data = pd.read_csv(metrics_data_path)\n",
                "    \n",
                "    # Load conversation data\n",
                "    conversation_data_path = sim_path / \"conversation_data.csv\"\n",
                "    conversation_data = None\n",
                "    if conversation_data_path.exists():\n",
                "        conversation_data = pd.read_csv(conversation_data_path)\n",
                "    \n",
                "    return {\n",
                "        \"dir_name\": simulation_dir,\n",
                "        \"metrics_summary\": metrics_summary,\n",
                "        \"metrics_data\": metrics_data,\n",
                "        \"conversation_data\": conversation_data\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def calculate_metrics_from_data(metrics_data):\n",
                "    \"\"\"Calculate metrics from raw metrics data\"\"\"\n",
                "    if metrics_data is None or len(metrics_data) == 0:\n",
                "        return None\n",
                "    \n",
                "    # Extract parameters\n",
                "    total_runs = metrics_data['run'].max()\n",
                "    total_months = metrics_data.groupby('run')['month'].max().max()\n",
                "    \n",
                "    # Get fishermen names\n",
                "    fishermen_cols = [col for col in metrics_data.columns if col.endswith('_caught')]\n",
                "    fishermen_names = [col.replace('_caught', '') for col in fishermen_cols]\n",
                "    \n",
                "    # 1. Survival Time\n",
                "    last_months = metrics_data.groupby(\"run\")[\"month\"].max().reset_index()\n",
                "    survival_times = last_months[\"month\"]\n",
                "    survival_time = survival_times.mean()\n",
                "    \n",
                "    # 2. Survival Rate\n",
                "    max_survival_runs = sum(survival_times == total_months)\n",
                "    survival_rate = (max_survival_runs / total_runs) * 100\n",
                "    \n",
                "    # 3. Total Gain for each agent\n",
                "    total_gains = {}\n",
                "    for name in fishermen_names:\n",
                "        total_gains[name] = metrics_data[f\"{name}_caught\"].sum()\n",
                "    \n",
                "    # Average gain\n",
                "    avg_gain = sum(total_gains.values()) / len(fishermen_names)\n",
                "    \n",
                "    # 4. Efficiency\n",
                "    # We'll use the sustainability threshold from the data\n",
                "    initial_sustainability_threshold = metrics_data.loc[metrics_data['month'] == 1, 'sustainability_threshold'].iloc[0]\n",
                "    total_catch = metrics_data[\"total_caught\"].sum()\n",
                "    max_efficiency = total_runs * total_months * initial_sustainability_threshold\n",
                "    efficiency = 1 - max(0, (max_efficiency - total_catch)) / max_efficiency\n",
                "    efficiency = efficiency * 100  # Convert to percentage\n",
                "    \n",
                "    # 5. Inequality (Gini coefficient)\n",
                "    total_gain_values = list(total_gains.values())\n",
                "    sum_abs_diffs = 0\n",
                "    total_sum = sum(total_gain_values)\n",
                "    \n",
                "    for i in range(len(total_gain_values)):\n",
                "        for j in range(len(total_gain_values)):\n",
                "            sum_abs_diffs += abs(total_gain_values[i] - total_gain_values[j])\n",
                "    \n",
                "    inequality = 1 - (sum_abs_diffs / (2 * len(total_gain_values) * total_sum)) if total_sum > 0 else 0\n",
                "    inequality = inequality * 100  # Convert to percentage\n",
                "    \n",
                "    # 6. Over-usage\n",
                "    over_threshold_cols = [f\"{name}_over_threshold\" for name in fishermen_names]\n",
                "    if all(col in metrics_data.columns for col in over_threshold_cols):\n",
                "        over_threshold_actions = metrics_data[over_threshold_cols].sum().sum()\n",
                "        total_actions = len(metrics_data) * len(fishermen_names)\n",
                "        over_usage = (over_threshold_actions / total_actions) * 100 if total_actions > 0 else 0\n",
                "    else:\n",
                "        # If over_threshold columns don't exist, calculate based on sustainability threshold\n",
                "        over_usage = None\n",
                "    \n",
                "    return {\n",
                "        \"survival_time\": survival_time,\n",
                "        \"survival_rate\": survival_rate,\n",
                "        \"average_gain\": avg_gain,\n",
                "        \"individual_gains\": total_gains,\n",
                "        \"efficiency\": efficiency,\n",
                "        \"equality\": inequality,\n",
                "        \"over_usage\": over_usage\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def analyze_individual_simulation(simulation_dir):\n",
                "    \"\"\"Analyze and visualize metrics for an individual simulation\"\"\"\n",
                "    sim_data = load_simulation_data(simulation_dir)\n",
                "    \n",
                "    # Check if we have data\n",
                "    if sim_data[\"metrics_data\"] is None:\n",
                "        print(f\"No metrics data found for {simulation_dir}\")\n",
                "        return\n",
                "    \n",
                "    # Use pre-calculated metrics if available, otherwise calculate\n",
                "    metrics = sim_data[\"metrics_summary\"]\n",
                "    if metrics is None:\n",
                "        metrics = calculate_metrics_from_data(sim_data[\"metrics_data\"])\n",
                "        if metrics is None:\n",
                "            print(f\"Could not calculate metrics for {simulation_dir}\")\n",
                "            return\n",
                "    \n",
                "    # Print metrics summary\n",
                "    print(f\"\\n--- Metrics for {simulation_dir} ---\")\n",
                "    print(f\"Survival Time: {metrics['survival_time']:.2f}\")\n",
                "    print(f\"Survival Rate: {metrics['survival_rate']:.2f}%\")\n",
                "    print(f\"Average Gain: {metrics['average_gain']:.2f}\")\n",
                "    print(f\"Efficiency: {metrics['efficiency']:.2f}%\")\n",
                "    print(f\"Equality: {metrics['equality']:.2f}%\")\n",
                "    print(f\"Over-usage: {metrics['over_usage']:.2f}%\")\n",
                "    \n",
                "    # Create visualizations for the simulation\n",
                "    metrics_data = sim_data[\"metrics_data\"]\n",
                "    \n",
                "    # Plot 1: Fish population over time\n",
                "    plt.figure(figsize=(12, 6))\n",
                "    \n",
                "    # Group by run and month, then plot fish before fishing\n",
                "    for run in range(1, metrics_data['run'].max() + 1):\n",
                "        run_data = metrics_data[metrics_data['run'] == run]\n",
                "        plt.plot(run_data['month'], run_data['fish_before'], marker='o', label=f'Run {run}')\n",
                "    \n",
                "    plt.axhline(y=5, color='r', linestyle='--', label='Collapse Threshold')\n",
                "    plt.xlabel('Month')\n",
                "    plt.ylabel('Fish Population')\n",
                "    plt.title('Fish Population Over Time')\n",
                "    plt.legend()\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    # Plot 2: Fishing decisions by agent\n",
                "    # Get fishermen names\n",
                "    fishermen_cols = [col for col in metrics_data.columns if col.endswith('_caught')]\n",
                "    fishermen_names = [col.replace('_caught', '') for col in fishermen_cols]\n",
                "    \n",
                "    plt.figure(figsize=(12, 6))\n",
                "    \n",
                "    # Get average catch by fisherman by month across all runs\n",
                "    avg_catches = metrics_data.groupby('month')[fishermen_cols].mean()\n",
                "    \n",
                "    # Plot average catch for each fisherman\n",
                "    for i, name in enumerate(fishermen_names):\n",
                "        plt.plot(avg_catches.index, avg_catches[f\"{name}_caught\"], marker='o', label=name)\n",
                "    \n",
                "    plt.xlabel('Month')\n",
                "    plt.ylabel('Average Fish Caught')\n",
                "    plt.title('Average Fishing Decisions by Agent')\n",
                "    plt.legend()\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    # Plot 3: Sustainability threshold and total catch\n",
                "    plt.figure(figsize=(12, 6))\n",
                "    \n",
                "    # Group by month, calculate average across runs\n",
                "    monthly_avg = metrics_data.groupby('month')[['total_caught', 'sustainability_threshold']].mean()\n",
                "    \n",
                "    plt.plot(monthly_avg.index, monthly_avg['total_caught'], marker='o', label='Total Caught')\n",
                "    plt.plot(monthly_avg.index, monthly_avg['sustainability_threshold'], marker='s', label='Sustainability Threshold')\n",
                "    \n",
                "    plt.xlabel('Month')\n",
                "    plt.ylabel('Fish')\n",
                "    plt.title('Total Catch vs Sustainability Threshold')\n",
                "    plt.legend()\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    # Plot 4: Individual gains by fisherman\n",
                "    plt.figure(figsize=(10, 6))\n",
                "    \n",
                "    individual_gains = metrics['individual_gains']\n",
                "    plt.bar(individual_gains.keys(), individual_gains.values())\n",
                "    plt.axhline(y=metrics['average_gain'], color='r', linestyle='--', label='Average')\n",
                "    \n",
                "    plt.xlabel('Fisherman')\n",
                "    plt.ylabel('Total Gain')\n",
                "    plt.title('Individual Gains by Fisherman')\n",
                "    plt.legend()\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    return metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Get all simulation directories\n",
                "simulation_dirs = [d for d in os.listdir(RESULTS_DIR) if os.path.isdir(RESULTS_DIR / d) and d != '__pycache__']\n",
                "print(f\"Found {len(simulation_dirs)} simulation directories: {simulation_dirs}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Analyze a specific simulation - replace with actual directory name\n",
                "if len(simulation_dirs) > 0:\n",
                "    analyze_individual_simulation(simulation_dirs[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Comparing Multiple Simulations\n",
                "\n",
                "This section compares metrics across different simulation runs to identify patterns and trends."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def collect_simulation_metrics():\n",
                "    \"\"\"Collect metrics from all simulations into a dataframe\"\"\"\n",
                "    all_metrics = []\n",
                "    \n",
                "    # Get all simulation directories\n",
                "    simulation_dirs = [d for d in os.listdir(RESULTS_DIR) if os.path.isdir(RESULTS_DIR / d) and d != '__pycache__']\n",
                "    \n",
                "    for sim_dir in simulation_dirs:\n",
                "        sim_data = load_simulation_data(sim_dir)\n",
                "        \n",
                "        # Check if metrics summary exists, otherwise calculate\n",
                "        metrics = sim_data[\"metrics_summary\"]\n",
                "        if metrics is None and sim_data[\"metrics_data\"] is not None:\n",
                "            metrics = calculate_metrics_from_data(sim_data[\"metrics_data\"])\n",
                "        \n",
                "        if metrics is not None:\n",
                "            # Parse directory name to extract model and settings\n",
                "            dir_parts = sim_dir.split('_')\n",
                "            if len(dir_parts) >= 6:\n",
                "                model_name = dir_parts[0]\n",
                "                social_memory = \"Enabled\" in dir_parts[1]\n",
                "                inheritance = \"Enabled\" in dir_parts[2]\n",
                "                lake_capacity = int(dir_parts[3]) if dir_parts[3].isdigit() else None\n",
                "                num_fishermen = int(dir_parts[4]) if dir_parts[4].isdigit() else None\n",
                "                num_months = int(dir_parts[5]) if dir_parts[5].isdigit() else None\n",
                "                num_runs = int(dir_parts[6]) if len(dir_parts) > 6 and dir_parts[6].isdigit() else None\n",
                "            else:\n",
                "                # If directory format is different, try to extract model name at least\n",
                "                model_name = dir_parts[0] if len(dir_parts) > 0 else \"unknown\"\n",
                "                social_memory = None\n",
                "                inheritance = None\n",
                "                lake_capacity = None\n",
                "                num_fishermen = None\n",
                "                num_months = None\n",
                "                num_runs = None\n",
                "            \n",
                "            # Create a record for this simulation\n",
                "            sim_record = {\n",
                "                \"Model\": model_name,\n",
                "                \"SocialMemory\": social_memory,\n",
                "                \"Inheritance\": inheritance,\n",
                "                \"LakeCapacity\": lake_capacity,\n",
                "                \"NumFishermen\": num_fishermen,\n",
                "                \"NumMonths\": num_months,\n",
                "                \"NumRuns\": num_runs,\n",
                "                \"SurvivalRate\": metrics[\"survival_rate\"],\n",
                "                \"SurvivalTime\": metrics[\"survival_time\"],\n",
                "                \"AverageGain\": metrics[\"average_gain\"],\n",
                "                \"Efficiency\": metrics[\"efficiency\"],\n",
                "                \"Equality\": metrics[\"equality\"],\n",
                "                \"OverUsage\": metrics[\"over_usage\"],\n",
                "                \"Directory\": sim_dir\n",
                "            }\n",
                "            all_metrics.append(sim_record)\n",
                "    \n",
                "    # Create DataFrame\n",
                "    metrics_df = pd.DataFrame(all_metrics)\n",
                "    return metrics_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Collect all metrics\n",
                "metrics_df = collect_simulation_metrics()\n",
                "\n",
                "# Check if we have data\n",
                "if len(metrics_df) == 0:\n",
                "    print(\"No metrics data found in any simulation directories\")\n",
                "else:\n",
                "    # Display the comparison table\n",
                "    print(f\"Metrics comparison across {len(metrics_df)} simulations:\\n\")\n",
                "    display(metrics_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def generate_comparison_visualizations(metrics_df):\n",
                "    \"\"\"Generate visualizations comparing metrics across simulations\"\"\"\n",
                "    if len(metrics_df) == 0:\n",
                "        print(\"No metrics data available for visualization\")\n",
                "        return\n",
                "    \n",
                "    # 1. Model comparison chart for key metrics\n",
                "    metrics_to_plot = ['SurvivalRate', 'SurvivalTime', 'Efficiency', 'Equality', 'OverUsage']\n",
                "    \n",
                "    # Group by model and calculate mean of metrics\n",
                "    model_metrics = metrics_df.groupby('Model')[metrics_to_plot].mean().reset_index()\n",
                "    \n",
                "    # Create a comparison chart for each metric\n",
                "    for metric in metrics_to_plot:\n",
                "        plt.figure(figsize=(10, 6))\n",
                "        sns.barplot(x='Model', y=metric, data=model_metrics)\n",
                "        plt.title(f'{metric} by Model')\n",
                "        plt.xlabel('Model')\n",
                "        plt.ylabel(metric)\n",
                "        plt.xticks(rotation=45)\n",
                "        plt.tight_layout()\n",
                "        plt.show()\n",
                "    \n",
                "    # 2. Social Memory and Inheritance effect\n",
                "    # Filter only rows where these settings are defined\n",
                "    settings_df = metrics_df.dropna(subset=['SocialMemory', 'Inheritance'])\n",
                "    \n",
                "    if len(settings_df) > 0:\n",
                "        # Create a 2x2 grid for SocialMemory and Inheritance settings\n",
                "        for metric in metrics_to_plot:\n",
                "            plt.figure(figsize=(12, 8))\n",
                "            \n",
                "            # Create a grouped bar chart\n",
                "            ax = sns.catplot(\n",
                "                data=settings_df, kind=\"bar\",\n",
                "                x=\"SocialMemory\", y=metric, hue=\"Inheritance\",\n",
                "                ci=\"sd\", palette=\"dark\", alpha=.6, height=6, aspect=1.5\n",
                "            )\n",
                "            \n",
                "            plt.title(f'Effect of Social Memory and Inheritance on {metric}')\n",
                "            plt.tight_layout()\n",
                "            plt.show()\n",
                "    \n",
                "    # 3. Correlation matrix of metrics\n",
                "    plt.figure(figsize=(10, 8))\n",
                "    correlation_metrics = ['SurvivalRate', 'SurvivalTime', 'AverageGain', 'Efficiency', 'Equality', 'OverUsage']\n",
                "    corr_matrix = metrics_df[correlation_metrics].corr()\n",
                "    \n",
                "    # Create heatmap of correlation matrix\n",
                "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
                "    plt.title('Correlation Matrix of Metrics')\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    # 4. Scatter plot matrix for key metrics\n",
                "    if len(metrics_df) >= 3:  # Only create if we have enough data points\n",
                "        sns.pairplot(metrics_df[correlation_metrics])\n",
                "        plt.suptitle('Relationships Between Metrics', y=1.02)\n",
                "        plt.tight_layout()\n",
                "        plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Generate comparison visualizations if we have data\n",
                "if len(metrics_df) > 0:\n",
                "    generate_comparison_visualizations(metrics_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def generate_comparison_table(metrics_df):\n",
                "    \"\"\"Generate a formatted comparison table like the one in the requirements\"\"\"\n",
                "    if len(metrics_df) == 0:\n",
                "        print(\"No metrics data available for table generation\")\n",
                "        return\n",
                "    \n",
                "    # Group by Model and calculate mean and std for each metric\n",
                "    metrics_columns = ['SurvivalRate', 'SurvivalTime', 'AverageGain', 'Efficiency', 'Equality', 'OverUsage']\n",
                "    \n",
                "    # Calculate mean and std for each model\n",
                "    grouped = metrics_df.groupby('Model')\n",
                "    \n",
                "    # Create a list to hold the formatted results\n",
                "    formatted_results = []\n",
                "    \n",
                "    for model, group in grouped:\n",
                "        row = {'Model': model}\n",
                "        \n",
                "        for metric in metrics_columns:\n",
                "            mean_val = group[metric].mean()\n",
                "            std_val = group[metric].std()\n",
                "            # Format as in the requirement: mean±std\n",
                "            row[metric] = f\"{mean_val:.1f}±{std_val:.2f}\"\n",
                "        \n",
                "        formatted_results.append(row)\n",
                "    \n",
                "    # Create DataFrame from formatted results\n",
                "    formatted_df = pd.DataFrame(formatted_results)\n",
                "    \n",
                "    # Reorder columns to match requirement\n",
                "    column_order = ['Model', 'SurvivalRate', 'SurvivalTime', 'AverageGain', 'Efficiency', 'Equality', 'OverUsage']\n",
                "    formatted_df = formatted_df[column_order]\n",
                "    \n",
                "    # Rename columns to match requirement\n",
                "    column_names = {\n",
                "        'SurvivalRate': 'Survival Rate',\n",
                "        'SurvivalTime': 'Survival Time',\n",
                "        'AverageGain': 'Gain',\n",
                "        'Efficiency': 'Efficiency',\n",
                "        'Equality': 'Equality',\n",
                "        'OverUsage': 'Over-usage'\n",
                "    }\n",
                "    formatted_df = formatted_df.rename(columns=column_names)\n",
                "    \n",
                "    # Display the table\n",
                "    return formatted_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Generate and display the formatted comparison table\n",
                "if len(metrics_df) > 0:\n",
                "    formatted_table = generate_comparison_table(metrics_df)\n",
                "    print(\"Comparison of LLM Models on GovSim Metrics:\\n\")\n",
                "    display(formatted_table)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}